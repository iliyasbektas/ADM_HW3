{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Reviewing Lab Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from webdriver_manager.firefox import GeckoDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "#import spacy\n",
    "from tqdm import tqdm\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "remove_punctuation = string.punctuation + '..' + '...' + \"''\" + \"``\"\n",
    "from collections import Counter\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n",
    "# driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "# driver = webdriver.Chrome(executable_path=ChromeDriverManager.install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, encoding='utf8') as infile:\n",
    "    html = BeautifulSoup(infile, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"http://www.goodreads.com/book/show/9222475-infernal-devices\")\n",
    "page.content\n",
    "with open(page.content, encoding='utf8') as infile:\n",
    "    soup = BeautifulSoup(infile, \"html.parser\")\n",
    "soup.prettify\n",
    "f = open(\"page.html\", \"w\")\n",
    "f.write(soup.prettify())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_book(href, driver, nlp):\n",
    "    driver.get(href)       \n",
    "    time.sleep(5)\n",
    "    page_soup = BeautifulSoup(driver.page_source, features=\"lxml\")\n",
    "    book_title = page_soup.find_all('h1')[0].contents[0].replace('\\n', '').strip()\n",
    "    author = page_soup.find_all('span', itemprop='name')[0].contents[0]\n",
    "    description = ' '.join([c for c in page_soup.find_all('div', id=\"description\")[0].contents[1].contents if isinstance(c, str)])\n",
    "    ratings = page_soup.find_all('a', href=\"#other_reviews\")\n",
    "    rating_count = -1\n",
    "    rating = -1\n",
    "    for raiting in ratings:\n",
    "        if raiting.find_all('meta', itemprop=\"ratingCount\"):\n",
    "            rating_count = raiting.text.replace('\\n', '').strip().split(' ')[0]\n",
    "        elif raiting.find_all('meta', itemprop=\"reviewCount\"):\n",
    "            rating = raiting.text.replace('\\n', '').strip().split(' ')[0]\n",
    "    doc = nlp(description)\n",
    "    token_list = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return (book_title, author, rating_count, rating, description, token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "### 1.1. Get the list of books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Firstly we will be working with one page to understand how it has to  be done. Since goodreads is blocked for some reason in Kazakhstan, I will try to investigate the goodreads by using 'requests' library. I will save then it to html to see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1\")\n",
    "soup_page = BeautifulSoup(page.content, features=\"lxml\")\n",
    "url = soup_page.find_all('div', class_=\"js-tooltipTrigger tooltipTrigger\") # here we are setting some parameters to choose books\n",
    "# as a result we obtain a dictionary for all first 100 books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"page.html\", \"w\")\n",
    "f.write(soup_page.prettify())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here we can see the first book, 'Hunger Games'. We have to work with href part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"js-tooltipTrigger tooltipTrigger\" data-resource-id=\"2767052\" data-resource-type=\"Book\">\n",
       "<a href=\"/book/show/2767052-the-hunger-games\" title=\"The Hunger Games\">\n",
       "<img alt=\"The Hunger Games (The Hunge...\" class=\"bookCover\" itemprop=\"image\" src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1586722975i/2767052._SY75_.jpg\"/>\n",
       "</a> </div>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thus, it becomes clear that we have to run for loops two times - for all 100 urls of first 300 pages.Then, we will have 30, 000 books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running our for loop, let's check our logic for one url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.goodreads.com/book/show/2767052-the-hunger-games']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list = []\n",
    "url_str = str(url[0]) \n",
    "url_str = url_str.split(\" \")\n",
    "url_str = url_str[5] \n",
    "url_str = url_str.split(\"\\\"\")[1]\n",
    "url_list += [\"https://www.goodreads.com\" + url_str]\n",
    "url_list   #looks alright "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "top_books = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"\n",
    "for _ in range (1,301):   # pages\n",
    "    page = requests.get(top_books + str(_))\n",
    "    soup_page = BeautifulSoup(page.content, features=\"lxml\")\n",
    "    url = soup.find_all('div', class_=\"js-tooltipTrigger tooltipTrigger\")\n",
    "    \n",
    "    for i in range (0,100): # books\n",
    "        url_str = str(url[i]) \n",
    "        url_str = url_str.split(\" \")\n",
    "        url_str = url_str[5] \n",
    "        url_str = url_str.split(\"\\\"\")[1]\n",
    "        url_list += [\"https://www.goodreads.com\" + url_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('url_list.txt', 'w') as f:\n",
    "    for item in url_list:\n",
    "        f.write(\"%s\\n\" % item) # saving it to txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have saved our list to a  txt file with 30 thousand books. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Book crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this section I am saving all urls to htmls. This is a part that took the most memory and time. Therefore, I used multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>https://www.goodreads.com/book/show/2.Harry_Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>https://www.goodreads.com/book/show/2657.To_Ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>https://www.goodreads.com/book/show/1885.Pride...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>https://www.goodreads.com/book/show/41865.Twil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url\n",
       "0  https://www.goodreads.com/book/show/2767052-th...\n",
       "1  https://www.goodreads.com/book/show/2.Harry_Po...\n",
       "2  https://www.goodreads.com/book/show/2657.To_Ki...\n",
       "3  https://www.goodreads.com/book/show/1885.Pride...\n",
       "4  https://www.goodreads.com/book/show/41865.Twil..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('url_list.txt', names=['url'], header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, link in list(enumerate(df['url'][4:20])):\n",
    "#     urllib.request.urlretrieve(link, r'C:\\\\Users\\\\User\\\\Desktop\\\\DS.Sapienza\\\\Data Mining. Aris\\\\HW3\\\\mein_arbeit\\\\html_data\\\\{}_book_{}.html'.format(link.split('/')[-1],str(idx)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_usage(idx, link):\n",
    "    urllib.request.urlretrieve(link, r\"/home/hduser/nfs/IliyasSpace/HW_Sapienza/Algorithmic Methods Of Data Mining And Laboratory (Aris)/HW3/html_data/{}_book_{}.html\".format(link.split('/')[-1],str(idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/hduser/nfs/IliyasSpace/HW_Sapienza/Algorithmic Methods Of Data Mining And Laboratory (Aris)/HW3/html_data/41865.Twilight_book_4.html',\n",
       " <http.client.HTTPMessage at 0x7f8e05fba278>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_usage(4, 'https://www.goodreads.com/book/show/41865.Twilight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "with Pool(cpu_count() - 6) as pool:\n",
    "    for idx, link in list(enumerate(df['url'][0:30000])):\n",
    "        pool.apply_async(cpu_usage, (idx, link))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After three hours we get almost all of our html files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this part we are iterating through our html files to create a dataframe that will further be used for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_taking(file):      \n",
    "    f = open(file, encoding='utf8')\n",
    "    \n",
    "#     with open(file, encoding='utf8') as infile:\n",
    "    soup = BeautifulSoup(f.read(), features='lxml')  \n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    Series = soup.find(id=\"bookSeries\")\n",
    "    if Series:\n",
    "        Series = Series.text.replace('(', '').replace(')', '').strip()\n",
    "    \n",
    "    page_number = soup.find(itemprop=\"numberOfPages\")\n",
    "    if page_number:\n",
    "        page_number = int(page_number.text.split()[0])\n",
    "  \n",
    "    Authors = soup.find('span', itemprop='name')\n",
    "    if Authors:\n",
    "        Authors = Authors.text\n",
    "    else:\n",
    "        print('', file)\n",
    "        return\n",
    "    \n",
    "    bookTitle = soup.find(id=\"bookTitle\")\n",
    "    if bookTitle:\n",
    "        bookTitle = bookTitle.text.replace('\\n', '').strip()\n",
    "    else:\n",
    "        print('', file)\n",
    "        return\n",
    "\n",
    "\n",
    "    rating = soup.find(itemprop=\"ratingValue\")\n",
    "    if rating:\n",
    "        rating = float(soup.find(itemprop=\"ratingValue\").text)\n",
    "    ratingCount = soup.find(itemprop=\"ratingCount\")\n",
    "    if ratingCount:\n",
    "        ratingCount = int(ratingCount.get('content'))\n",
    "  \n",
    "    reviewCount = soup.find(itemprop=\"reviewCount\")\n",
    "  \n",
    "    if reviewCount: \n",
    "        reviewCount = int(reviewCount.get('content'))\n",
    "    Plot = soup.find(id=\"description\")\n",
    "    if Plot:\n",
    "        Plot = (Plot).find_all('span')[-1].text \n",
    "        try:\n",
    "            if detect(Plot) != 'en': #check if is a English plot\n",
    "                return\n",
    "        except:\n",
    "              pass\n",
    "\n",
    "    try:\n",
    "        publish = (soup.find(id=\"details\")).find_all('div')[1].text.split('\\n')[2].strip()\n",
    "    except:\n",
    "        publish = None\n",
    "    try:\n",
    "        characters = ', '.join(map (lambda x : x.text,  (soup.find(id=\"bookDataBox\")).find_all('a', href = re.compile(\"^.characters\"))))\n",
    "    except:\n",
    "        characters = None\n",
    "    try:    \n",
    "        settings = ', '.join(map (lambda x : x.text.replace(',',''),  (soup.find(id=\"bookDataBox\")).find_all('a', href = re.compile(\"^.places\"))))\n",
    "    except:\n",
    "        settings = None\n",
    "    try:\n",
    "        link = soup.find('link', itemprop='url').get('href')\n",
    "    except:\n",
    "        link = None\n",
    "        \n",
    "        \n",
    "    return [bookTitle, Series, Authors, rating, Plot, ratingCount, reviewCount, page_number, publish, characters, settings, link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Marshmallow Madness!: Dozens of Puffalicious Recipes',\n",
       " '',\n",
       " 'Shauna Sever',\n",
       " 4.21,\n",
       " 'If you’ve never tasted a fresh, homemade marshmallow, are you in for a treat! Marshmallow Madness! shows how to whip up dozens of fluffy, puffy flavors—from Strawberry and Vanilla to Buttered Rum, Root Beer Float, Maple Bacon, and more. Author Shauna Sever also includes easy recipes for homemade graham crackers, drinks for mallow dunking, and a host of irresistible desserts, including Lemon Dream Whoopie Pies, Blonde Rocky Road, and S’mores Cupcakes.',\n",
       " 273,\n",
       " 20,\n",
       " 96,\n",
       " 'February 28th 2012',\n",
       " '',\n",
       " '',\n",
       " 'https://www.goodreads.com/review/show/303120196']"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_taking(\"/home/hduser/nfs/IliyasSpace/HW_Sapienza/Algorithmic Methods Of Data Mining And Laboratory (Aris)/HW3/html_data2/13069215-marshmallow-madness_book_29988.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_list = []\n",
    "# directory = 'C:\\\\Users\\\\User\\\\Desktop\\\\DS.Sapienza\\\\Data Mining. Aris\\\\HW3\\\\mein_arbeit\\\\html_data'\n",
    "# for content in os.listdir(directory):\n",
    "#     content_list.append(os.path.join(directory,content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have to create a list of unique pathes of our html files in order to iterate them through parsing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = []\n",
    "directory = \"/home/hduser/nfs/IliyasSpace/HW_Sapienza/Algorithmic Methods Of Data Mining And Laboratory (Aris)/HW3/html_data\"\n",
    "for content in os.listdir(directory):\n",
    "    content_list.append(os.path.join(directory,content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(columns  = ['bookTitle','bookSeries', 'bookAuthors', \\\n",
    "                                    'ratingValue', 'Plot', 'ratingCount','reviewCount' ,\\\n",
    "                                    'NumberofPages','PublishingDate', 'Characters','Setting', 'url'])\n",
    "#[bookTitle, bookSeries, bookAuthors, ratingValue, Plot, ratingCount, reviewCount, Number_of_Pages,Published,Characters,Settings,link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(cpu_count() - 25) as pool:\n",
    "    df_final = pd.DataFrame(columns  = ['bookTitle','bookSeries', 'bookAuthors', \\\n",
    "                                    'ratingValue', 'Plot', 'ratingCount','reviewCount' ,\\\n",
    "                                    'NumberofPages','PublishingDate', 'Characters','Setting', 'url'])\n",
    "    for book in content_list:\n",
    "        pool.apply_async(extract_info, (df_final, book))\n",
    "        \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in content_list:\n",
    "    info = information_taking(book)\n",
    "    a_series = pd.Series(info, index = df_final.columns)\n",
    "    df_final = df_final.append(a_series, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, we will gather information about all our htmls into one dataframe again either by using multiprocessing or just using for loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since it took too much space, I saved all functions for this section in a separate python file which you will see in a directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is basically cleaning the information about plot in data. It removes stops and punctuations. It does the stemming and etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 15/15 [00:00<00:00, 234.47it/s]\n"
     ]
    }
   ],
   "source": [
    "df_final['tokenized_dscr'] = df_final['Plot'].progress_apply(lambda x: prepare_search(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our inverse index we have to firstly store in the words list all the token we have met tokenizing the plot. Then, sorting by alphabetical order is done.\n",
    "In the dictionary, we create a key-value relationship so that we have a token as a key and an index of that particular word as a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 15/15 [00:00<00:00, 2148.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 804/804 [00:00<00:00, 803866.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 804/804 [00:00<00:00, 804058.28it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "15it [00:00, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "15it [00:00, 15004.66it/s]\n"
     ]
    }
   ],
   "source": [
    "inv_idx, voc, vocab = dict_prep(df_final['tokenized_dscr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our token and key information, we are able to find common words in plots. Basically,this is what this function does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The Fault in Our Stars</td>\n",
       "      <td>Despite the tumor-shrinking medical miracle th...</td>\n",
       "      <td>https://www.goodreads.com/review/show/289575457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The Book Thief</td>\n",
       "      <td>Librarian's note: An alternate cover edition c...</td>\n",
       "      <td>https://www.goodreads.com/review/show/387598058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>The unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>https://www.goodreads.com/review/show/94754648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Could you survive on your own in the wild, wit...</td>\n",
       "      <td>https://www.goodreads.com/review/show/122569940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>The Giving Tree</td>\n",
       "      <td>\"Once there was a tree...and she loved a littl...</td>\n",
       "      <td>https://www.goodreads.com/review/show/133528388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                bookTitle                                               Plot  \\\n",
       "1  The Fault in Our Stars  Despite the tumor-shrinking medical miracle th...   \n",
       "4          The Book Thief  Librarian's note: An alternate cover edition c...   \n",
       "6   To Kill a Mockingbird  The unforgettable novel of a childhood in a sl...   \n",
       "7        The Hunger Games  Could you survive on your own in the wild, wit...   \n",
       "9         The Giving Tree  \"Once there was a tree...and she loved a littl...   \n",
       "\n",
       "                                               url  \n",
       "1  https://www.goodreads.com/review/show/289575457  \n",
       "4  https://www.goodreads.com/review/show/387598058  \n",
       "6   https://www.goodreads.com/review/show/94754648  \n",
       "7  https://www.goodreads.com/review/show/122569940  \n",
       "9  https://www.goodreads.com/review/show/133528388  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query('love', df_final, inv_idx, voc).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second search engine, given a query, we want to get the top-k (the choice of k it's up to you!) documents related to the query. In particular:\n",
    "\n",
    "We will find all the documents that contains all the words in the query.\n",
    "We will Sort them by their similarity with the query\n",
    "Return in output k documents, or all the documents with non-zero similarity with the query when the results are less than k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 15/15 [00:00<00:00, 283.08it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 15/15 [00:00<00:00, 14775.61it/s]\n"
     ]
    }
   ],
   "source": [
    "df_final['tokenized_dscr'] = df_final['Plot'].progress_apply(lambda x: prepare_search2(x))\n",
    "df_final['tokenized_dscr2'] = df_final['tokenized_dscr'].progress_apply(lambda x: Counter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'journey': 1, 'end': 1, 'world': 2, 'fantast'...\n",
       "1    {'despit': 1, 'tumorshrink': 1, 'medic': 1, 'm...\n",
       "2    {'librarian': 1, 'note': 1, 'altern': 1, 'cove...\n",
       "3    {'scarlett': 1, 'ohara': 1, 'beauti': 1, 'spoi...\n",
       "4    {'librarian': 1, 'note': 1, 'altern': 1, 'cove...\n",
       "Name: tokenized_dscr2, dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.tokenized_dscr2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 15/15 [00:00<00:00, 15054.93it/s]\n"
     ]
    }
   ],
   "source": [
    "empty_list = []\n",
    "for n in tqdm(range(len(df_final[\"tokenized_dscr2\"]))):\n",
    "    empty_list.append(tf_score(df_final[\"tokenized_dscr2\"][n]))\n",
    "df_final['tf_score'] =empty_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>bookSeries</th>\n",
       "      <th>bookAuthors</th>\n",
       "      <th>ratingValue</th>\n",
       "      <th>Plot</th>\n",
       "      <th>ratingCount</th>\n",
       "      <th>reviewCount</th>\n",
       "      <th>NumberofPages</th>\n",
       "      <th>PublishingDate</th>\n",
       "      <th>Characters</th>\n",
       "      <th>Setting</th>\n",
       "      <th>url</th>\n",
       "      <th>tokenized_dscr</th>\n",
       "      <th>tokenized_dscr2</th>\n",
       "      <th>tf_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The Chronicles of Narnia</td>\n",
       "      <td>The Chronicles of Narnia Publication Order #1–7</td>\n",
       "      <td>C.S. Lewis</td>\n",
       "      <td>4.26</td>\n",
       "      <td>Journeys to the end of the world, fantastic cr...</td>\n",
       "      <td>524435</td>\n",
       "      <td>10441</td>\n",
       "      <td>767</td>\n",
       "      <td>September 16th 2002</td>\n",
       "      <td>Polly, Aslan, Lucy Pevensie, Edmund Pevensie, ...</td>\n",
       "      <td>London England</td>\n",
       "      <td>https://www.goodreads.com/review/show/2630144</td>\n",
       "      <td>[journey, end, world, fantast, creatur, epic, ...</td>\n",
       "      <td>{'journey': 1, 'end': 1, 'world': 2, 'fantast'...</td>\n",
       "      <td>{'journey': 0.01020408163265306, 'end': 0.0102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The Fault in Our Stars</td>\n",
       "      <td></td>\n",
       "      <td>John Green</td>\n",
       "      <td>4.20</td>\n",
       "      <td>Despite the tumor-shrinking medical miracle th...</td>\n",
       "      <td>3588818</td>\n",
       "      <td>156048</td>\n",
       "      <td>313</td>\n",
       "      <td>January 10th 2012</td>\n",
       "      <td>Hazel Grace Lancaster, Augustus Waters, Isaac</td>\n",
       "      <td>Indianapolis Indiana, Amsterdam</td>\n",
       "      <td>https://www.goodreads.com/review/show/289575457</td>\n",
       "      <td>[despit, tumorshrink, medic, miracl, bought, y...</td>\n",
       "      <td>{'despit': 1, 'tumorshrink': 1, 'medic': 1, 'm...</td>\n",
       "      <td>{'despit': 0.019230769230769232, 'tumorshrink'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  bookTitle                                       bookSeries  \\\n",
       "0  The Chronicles of Narnia  The Chronicles of Narnia Publication Order #1–7   \n",
       "1    The Fault in Our Stars                                                    \n",
       "\n",
       "  bookAuthors  ratingValue                                               Plot  \\\n",
       "0  C.S. Lewis         4.26  Journeys to the end of the world, fantastic cr...   \n",
       "1  John Green         4.20  Despite the tumor-shrinking medical miracle th...   \n",
       "\n",
       "  ratingCount reviewCount NumberofPages       PublishingDate  \\\n",
       "0      524435       10441           767  September 16th 2002   \n",
       "1     3588818      156048           313    January 10th 2012   \n",
       "\n",
       "                                          Characters  \\\n",
       "0  Polly, Aslan, Lucy Pevensie, Edmund Pevensie, ...   \n",
       "1      Hazel Grace Lancaster, Augustus Waters, Isaac   \n",
       "\n",
       "                           Setting  \\\n",
       "0                   London England   \n",
       "1  Indianapolis Indiana, Amsterdam   \n",
       "\n",
       "                                               url  \\\n",
       "0    https://www.goodreads.com/review/show/2630144   \n",
       "1  https://www.goodreads.com/review/show/289575457   \n",
       "\n",
       "                                      tokenized_dscr  \\\n",
       "0  [journey, end, world, fantast, creatur, epic, ...   \n",
       "1  [despit, tumorshrink, medic, miracl, bought, y...   \n",
       "\n",
       "                                     tokenized_dscr2  \\\n",
       "0  {'journey': 1, 'end': 1, 'world': 2, 'fantast'...   \n",
       "1  {'despit': 1, 'tumorshrink': 1, 'medic': 1, 'm...   \n",
       "\n",
       "                                            tf_score  \n",
       "0  {'journey': 0.01020408163265306, 'end': 0.0102...  \n",
       "1  {'despit': 0.019230769230769232, 'tumorshrink'...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 15/15 [00:00<00:00, 7509.50it/s]\n"
     ]
    }
   ],
   "source": [
    "dic = {}\n",
    "for a in tqdm(df_final['tokenized_dscr']):   \n",
    "    for b in a:\n",
    "         dic.update({ b : math.log(len(df_final)/ len(vocab[b]))})   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the new inverse index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 804/804 [00:00<00:00, 402316.92it/s]\n"
     ]
    }
   ],
   "source": [
    "n_inv_idx = new_inverse_index(inv_idx, voc, empty_list, dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine-similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ab = np.zeros(len(vocab))\n",
    "# for w in query:\n",
    "#     arr_q[voc[w]] = 1\n",
    "# cos_sim = {}\n",
    "# for doc in tqdm(s):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above should have been a code for cosine similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating an algorithm so that we have to find the maximum length of a subsequence of characters that is in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_question(string):\n",
    "    \n",
    "    if len(string)==1:\n",
    "        return [string]\n",
    "    else:\n",
    "        teta=alg_question(string[:len(string)-1])\n",
    "        for a in teta:\n",
    "            if string[-1]>a[-1]:\n",
    "                teta.append(a+string[-1])\n",
    "        teta.append(string[-1])\n",
    "        return teta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q', 'p', 'k', 'kl', 'l', 'd', 'kp', 'klp', 'lp', 'dp']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg_question('qpkldpoqwdpvksdmfsngse')[:10] # information of testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
